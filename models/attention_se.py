
import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import ImageEnhance, ImageFilter, Image
import matplotlib.pyplot as plt
from torchvision import transforms

distortion_map = {
    # âœ… CSIQ
    "awgn": 0,                     # ğŸŸ  Sobel (ë…¸ì´ì¦ˆ íŒ¨í„´ íƒì§€)
    "blur": 1,                     # ğŸŸ  Sobel (ê²½ê³„ íë¦¼ íƒì§€)
    "contrast": 2,                 # ğŸŸ¡ HSV / Histogram (ëª…ì•” ëŒ€ë¹„ ë¶„ì„)
    "fnoise": 3,                   # ğŸŸ  Sobel (ë…¸ì´ì¦ˆ íŒ¨í„´ íƒì§€)
    "jpeg": 4,                     # ğŸŸ¢ Fourier (ì••ì¶• ì¸ê³µë¬¼ ë¶„ì„)
    "jpeg2000": 5,                 # ğŸŸ¢ Fourier (ì••ì¶• ì¸ê³µë¬¼ ë¶„ì„)

    # âœ… KADID
    "gaussian_blur": 6,            # ğŸŸ  Sobel (ê²½ê³„ íë¦¼ íƒì§€)
    "lens_blur": 7,                # ğŸŸ  Sobel (ê²½ê³„ íë¦¼ íƒì§€)
    "motion_blur": 8,              # ğŸŸ  Sobel (ëª¨ì…˜ íë¦¼ íƒì§€)
    "color_diffusion": 9,          # ğŸŸ¡ HSV (ìƒ‰ìƒ í™•ì‚° ë¶„ì„)
    "color_shift": 10,             # ğŸŸ¡ HSV (ìƒ‰ìƒ ë³€í™” íƒì§€)
    "color_quantization": 11,      # ğŸŸ¢ Fourier (ì–‘ìí™” íŒ¨í„´ ë¶„ì„)
    "color_saturation_1": 12,      # ğŸŸ¡ HSV (ì±„ë„ ë¶„ì„)
    "color_saturation_2": 13,      # ğŸŸ¡ HSV (ì±„ë„ ë¶„ì„)
    "jpeg2000": 14,                # ğŸŸ¢ Fourier (ì••ì¶• ì¸ê³µë¬¼ ë¶„ì„)
    "jpeg": 15,                    # ğŸŸ¢ Fourier (ì••ì¶• ì¸ê³µë¬¼ ë¶„ì„)
    "white_noise": 16,             # ğŸŸ  Sobel (ë…¸ì´ì¦ˆ íŒ¨í„´ íƒì§€)
    "white_noise_color_component": 17,  # ğŸŸ  Sobel (ì»¬ëŸ¬ ë…¸ì´ì¦ˆ íŒ¨í„´ íƒì§€)
    "impulse_noise": 18,           # ğŸŸ  Sobel (ì¡ìŒ ì  íƒì§€)
    "multiplicative_noise": 19,    # ğŸŸ  Sobel (ë…¸ì´ì¦ˆ íŒ¨í„´ íƒì§€)
    "denoise": 20,                 # ğŸŸ¡ Histogram (ë…¸ì´ì¦ˆ ì œê±° í›„ ëŒ€ë¹„ ë¶„ì„)
    "brighten": 21,                # ğŸŸ¡ HSV / Histogram (ë°ê¸° ë³€í™” ë¶„ì„)
    "darken": 22,                  # ğŸŸ¡ HSV / Histogram (ëª…ì•” ë¶„ì„)
    "mean_shift": 23,              # ğŸŸ¢ Fourier (í‰ê·  ì´ë™ìœ¼ë¡œ ì¸í•œ ì£¼íŒŒìˆ˜ ë³€í™” íƒì§€)
    "jitter": 24,                  # ğŸŸ  Sobel (ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆ íŒ¨í„´ ë¶„ì„)
    "non_eccentricity_patch": 25,  # ğŸŸ  Sobel (íŒ¨ì¹˜ ê²½ê³„ ë¶„ì„)
    "pixelate": 26,                # ğŸŸ¢ Fourier (í”½ì…€í™” ì£¼íŒŒìˆ˜ íŒ¨í„´ ë¶„ì„)
    "quantization": 27,            # ğŸŸ¢ Fourier (ì–‘ìí™” ì£¼íŒŒìˆ˜ íŒ¨í„´ ë¶„ì„)
    "color_block": 28,             # ğŸŸ¢ Fourier (ìƒ‰ìƒ ë¸”ë¡ ì£¼íŒŒìˆ˜ ë¶„ì„)
    "high_sharpen": 29,            # ğŸŸ  Sobel (ê³¼ë„í•œ ê²½ê³„ ê°•ì¡° íƒì§€)
    "contrast_change": 30,         # ğŸŸ¡ HSV / Histogram (ëª…ì•” ëŒ€ë¹„ ë¶„ì„)

    # âœ… KONIQ
    "low-light_noise": 31,         # ğŸŸ  Sobel (ì €ì¡°ë„ ë…¸ì´ì¦ˆ íŒ¨í„´ ë¶„ì„)
    "underexposure": 32,           # ğŸŸ¡ Histogram (ì €ë…¸ì¶œ ë¶„ì„)
    "overexposure": 33,            # ğŸŸ¡ Histogram (ê³¼ë…¸ì¶œ ë¶„ì„)
    "sensor_noise": 34,            # ğŸŸ  Sobel (ì„¼ì„œ ë…¸ì´ì¦ˆ íŒ¨í„´ íƒì§€)
    "banding_artifacts": 35,       # ğŸŸ¢ Fourier (ì¤„ë¬´ëŠ¬ ì¸ê³µë¬¼ ë¶„ì„)
    "chromatic_aberration": 36,    # ğŸŸ¡ HSV (ìƒ‰ìˆ˜ì°¨ íƒì§€)
    "camera_motion_blur": 37,      # ğŸŸ  Sobel (ëª¨ì…˜ íë¦¼ ë¶„ì„)
    "moving_object_blur": 38,      # ğŸŸ  Sobel (ì›€ì§ì´ëŠ” ê°ì²´ íë¦¼ ë¶„ì„)
    "mixture_distortions": 39,     # ğŸŸ¢ Fourier (ë³µí•© ì£¼íŒŒìˆ˜ íŒ¨í„´ ë¶„ì„)

    # âœ… LIVE-Challenge
    "brightness": 40,              # ğŸŸ¡ HSV / Histogram (ë°ê¸° ë³€í™” ë¶„ì„)
    "exposure": 41,                # ğŸŸ¡ HSV / Histogram (ë…¸ì¶œ ë¶„ì„)
    "colorfulness": 42,            # ğŸŸ¡ HSV (ìƒ‰ì¡° ë° ì±„ë„ ë¶„ì„)
    "color_shift": 43,             # ğŸŸ¡ HSV (ìƒ‰ìƒ ì´ë™ íƒì§€)
    "white_balance_error": 44,     # ğŸŸ¡ HSV (ìƒ‰ ì˜¨ë„ ë¶„ì„)
    "sharpness": 45,               # ğŸŸ  Sobel (ì„ ëª…ë„ ë° ê²½ê³„ ë¶„ì„)
    "motion_blur": 46,             # ğŸŸ  Sobel (ëª¨ì…˜ íë¦¼ íƒì§€)
    "glare": 47,                   # ğŸŸ¡ Histogram (ì¡°ëª… ë°˜ì‚¬ íŒ¨í„´ ë¶„ì„)
    "haze": 48,                    # ğŸŸ¡ Histogram / Fourier (íë¦¼ ë° ì €ì£¼íŒŒ ë¶„ì„)
    "low-light_noise": 49,         # ğŸŸ  Sobel (ì €ì¡°ë„ ë…¸ì´ì¦ˆ ë¶„ì„)
    "color_noise": 50,             # ğŸŸ¡ HSV (ì»¬ëŸ¬ ë…¸ì´ì¦ˆ ë¶„ì„)
    "vignetting": 51,              # ğŸŸ¡ Histogram (ë¹„ë„¤íŒ… ë°ê¸° íŒ¨í„´ ë¶„ì„)
    "distortion": 52,              # ğŸŸ¢ Fourier (ë Œì¦ˆ ì™œê³¡ ì£¼íŒŒìˆ˜ ë¶„ì„)

    # âœ… SPAQ
    "under_exposure": 53,          # ğŸŸ¡ Histogram (ì €ë…¸ì¶œ ë¶„ì„)
    "over_exposure": 54,           # ğŸŸ¡ Histogram (ê³¼ë…¸ì¶œ ë¶„ì„)
    "sensor_noise": 55,            # ğŸŸ  Sobel (ì„¼ì„œ ë…¸ì´ì¦ˆ ë¶„ì„)
    "contrast_reduction": 56,      # ğŸŸ¡ Histogram (ëŒ€ë¹„ ê°ì†Œ ë¶„ì„)
    "out_of_focus": 57,            # ğŸŸ  Sobel (ì´ˆì  íë¦¼ íƒì§€)
    "camera_motion_blur": 58,      # ğŸŸ  Sobel (ì¹´ë©”ë¼ ëª¨ì…˜ íë¦¼ íƒì§€)
    "moving_object_blur": 59,      # ğŸŸ  Sobel (ì›€ì§ì´ëŠ” ê°ì²´ íë¦¼ íƒì§€)
    "mixture_distortions": 60,     # ğŸŸ¢ Fourier (ë³µí•© ì™œê³¡ ì£¼íŒŒìˆ˜ ë¶„ì„)

    # âœ… TID
    "additive_gaussian_noise": 61,              # ğŸŸ  Sobel (ë…¸ì´ì¦ˆ íŒ¨í„´ ë¶„ì„)
    "additive_noise_in_color_components": 62,   # ğŸŸ¡ HSV (ì»¬ëŸ¬ ë…¸ì´ì¦ˆ ë¶„ì„)
    "spatially_correlated_noise": 63,           # ğŸŸ  Sobel (ê³µê°„ì  ìƒê´€ ë…¸ì´ì¦ˆ íƒì§€)
    "masked_noise": 64,                         # ğŸŸ¡ Histogram (ë…¸ì´ì¦ˆ ë§ˆìŠ¤í‚¹ ë¶„ì„)
    "high_frequency_noise": 65,                 # ğŸŸ¢ Fourier (ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ë¶„ì„)
    "impulse_noise": 66,                        # ğŸŸ  Sobel (ì„í„ìŠ¤ ë…¸ì´ì¦ˆ íƒì§€)
    "quantization_noise": 67,                   # ğŸŸ¢ Fourier (ì–‘ìí™” ë…¸ì´ì¦ˆ ë¶„ì„)
    "image_denoising": 68,                      # ğŸŸ¡ Histogram (ë…¸ì´ì¦ˆ ì œê±° í›„ ë¶„ì„)
    "jpeg_compression": 69,                     # ğŸŸ¢ Fourier (JPEG ì••ì¶• íŒ¨í„´ ë¶„ì„)
    "jpeg2000_compression": 70,                 # ğŸŸ¢ Fourier (JPEG2000 ì••ì¶• ë¶„ì„)
    "jpeg_transmission_errors": 71,             # ğŸŸ¢ Fourier (ì „ì†¡ ì˜¤ë¥˜ ë¶„ì„)
    "jpeg2000_transmission_errors": 72,         # ğŸŸ¢ Fourier (ì „ì†¡ ì˜¤ë¥˜ ë¶„ì„)
    "non_eccentricity_pattern_noise": 73,       # ğŸŸ  Sobel (íŒ¨í„´ ë…¸ì´ì¦ˆ íƒì§€)
    "local_block_wise_distortions": 74,         # ğŸŸ¢ Fourier (ë¸”ë¡ ê¸°ë°˜ ì™œê³¡ ë¶„ì„)
    "mean_shift": 75,                           # ğŸŸ¢ Fourier (í‰ê·  ì´ë™ ì£¼íŒŒìˆ˜ ë¶„ì„)
    "change_of_color_saturation": 76,           # ğŸŸ¡ HSV (ì±„ë„ ë³€í™” íƒì§€)
    "multiplicative_gaussian_noise": 77,        # ğŸŸ  Sobel (ë‹¤ì¤‘ ë…¸ì´ì¦ˆ íƒì§€)
    "comfort_noise": 78,                        # ğŸŸ  Sobel (ë¶€ë“œëŸ¬ìš´ ë…¸ì´ì¦ˆ íŒ¨í„´ ë¶„ì„)
    "lossy_compression_of_noisy_images": 79,    # ğŸŸ¢ Fourier (ì†ì‹¤ ì••ì¶• íŒ¨í„´ ë¶„ì„)
    "image_color_quantization_with_dither": 80, # ğŸŸ¢ Fourier (ì»¬ëŸ¬ ì–‘ìí™” ë¶„ì„)
    "sparse_sampling_and_reconstruction": 81,   # ğŸŸ¢ Fourier (í¬ì†Œ ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ ë¶„ì„)
    "chromatic_aberrations": 82,                # ğŸŸ¡ HSV (ìƒ‰ìˆ˜ì°¨ íƒì§€)
}

# âœ… í•„í„° ìœ í˜• ìš”ì•½:
# ğŸŸ  Sobel: ê²½ê³„, ë…¸ì´ì¦ˆ, ë¸”ëŸ¬ íƒì§€
# ğŸŸ¡ HSV / Histogram: ë°ê¸°, ì±„ë„, ëŒ€ë¹„ ë° ìƒ‰ìƒ ë³€í™” ë¶„ì„
# ğŸŸ¢ Fourier: ì••ì¶• ì¸ê³µë¬¼, ê³ ì£¼íŒŒ/ì €ì£¼íŒŒ íŒ¨í„´ ë¶„ì„


# âœ… ì¶œë ¥
""" for distortion, idx in distortion_map.items():
    print(f"{idx}: {distortion}")
 """


class DistortionClassifier(nn.Module):
    def __init__(self, in_channels, num_distortions=83):
        super(DistortionClassifier, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.fc = nn.Linear(128, num_distortions)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc(x)
        return x


class AttributeFeatureProcessor(nn.Module):
    def __init__(self, in_channels):
        super(AttributeFeatureProcessor, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU()
        )

    def forward(self, x):
        return self.conv(x)

class TextureBlockProcessor(nn.Module):
    def __init__(self, in_channels):
        super(TextureBlockProcessor, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=2, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)
        )

    def forward(self, x):
        return self.conv(x)


class DistortionAttention(nn.Module):
    def __init__(self, in_channels, expected_channels=256):
        super(DistortionAttention, self).__init__()
        self.expected_channels = expected_channels
        query_key_channels = max(1, expected_channels // 8)
        
        # âœ… ì…ë ¥ ì±„ë„ì„ ResNetì˜ expectationê³¼ ë§ì¶¤
        self.channel_adjust = nn.Conv2d(in_channels, expected_channels, kernel_size=1)
        
        self.query_conv = nn.Conv2d(expected_channels, query_key_channels, kernel_size=1)
        self.key_conv = nn.Conv2d(expected_channels, query_key_channels, kernel_size=1)
        self.value_conv = nn.Conv2d(expected_channels, expected_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)
        self.distortion_classifier = DistortionClassifier(expected_channels)
    
    def forward(self, x):
        b, c, h, w = x.size()
        
        # âœ… ì±„ë„ ë³€í™˜ ì ìš©
        x = self.channel_adjust(x)  # 512 â†’ 256 ë³€í™˜
        
        # âœ… Attention Map ë¨¼ì € ê³„ì‚°
        query = self.query_conv(x).view(b, -1, h * w).permute(0, 2, 1)
        key = self.key_conv(x).view(b, -1, h * w)
        value = self.value_conv(x).view(b, -1, h * w)
        
        scale = query.size(-1) ** -0.5
        attention = self.softmax(torch.bmm(query, key) * scale)
        attention_map = torch.bmm(value, attention.permute(0, 2, 1)).view(b, self.expected_channels, h, w)
        
        # âœ… Distortion Classifierë¥¼ í†µí•´ ì™œê³¡ ìœ í˜• ì˜ˆì¸¡
        distortion_logits = self.distortion_classifier(x)
        distortion_types = torch.argmax(distortion_logits, dim=1)
        
        # âœ… Specialized Filter ì ìš©
        filtered_tensors = []
        for i, dt in enumerate(distortion_types):
            filtered_output = self._apply_filter(x[i].unsqueeze(0), dt)
            
            # íŠœí”Œ ë°˜í™˜ ì‹œ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš©
            if isinstance(filtered_output, tuple):
                filtered_x = filtered_output[0]
            else:
                filtered_x = filtered_output

            if filtered_x.shape[1] == 1:
                filtered_x = filtered_x.expand(-1, self.expected_channels, -1, -1)
            filtered_tensors.append(filtered_x)

        filtered_x = torch.cat(filtered_tensors, dim=0)

        
        # âœ… Attention Mapê³¼ í•„í„° ì ìš© ì •ë³´ ê²°í•©
        feature_map = attention_map + filtered_x  # ğŸ”¥ Feature Map ìƒì„± ë°©ì‹ ë³€ê²½
        
        return feature_map, distortion_logits  # âœ… ë¸”ë¡ë„ì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •



    def _apply_filter(self, x, distortion_type):
        """ ì™œê³¡ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ í•„í„° ì ìš© """
        
        # âœ… Sobel í•„í„° (ê²½ê³„ íƒì§€ ë° ë…¸ì´ì¦ˆ, ë¸”ëŸ¬ ë¶„ì„)
        if distortion_type in [
            0, 1, 3, 5, 6, 7, 8, 9, 10, 
            11, 12, 13, 14, 15, 16, 17, 18, 
            19, 24, 25, 26, 31, 34, 37, 
            38, 39, 40, 41, 43, 45, 46, 50, 
            55, 57, 59, 63, 64, 65, 66, 
            73
        ]:
            print(f"[Debug] Applying Sobel filter for {distortion_type}")
            return self._sobel_filter(x)

        # âœ… HSV/Histogram ë¶„ì„ (ìƒ‰ìƒ, ëª…ì•”, ë°ê¸°, ëŒ€ë¹„ íƒì§€)
        elif distortion_type in [
            2, 4, 10, 20, 21, 22, 23, 27, 
            28, 29, 30, 32, 33, 35, 42, 
            44, 47, 48, 49, 51, 53, 54, 
            56, 60, 62, 68, 76
        ]:
            print(f"[Debug] Applying HSV/Histogram analysis for {distortion_type}")
            return self._hsv_analysis(x) if distortion_type not in [20, 21, 51, 53, 56, 60] else self._histogram_analysis(x)

        # âœ… Fourier ë¶„ì„ (ì••ì¶• ì¸ê³µë¬¼ ë° ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë¶„ì„)
        elif distortion_type in [
            4, 14, 15, 45, 46, 47, 48, 49, 
            50, 52, 54, 58, 61, 67, 69, 
            70, 71, 72, 74, 75, 77, 78, 
            79, 80, 81
        ]:
            print(f"[Debug] Applying Fourier analysis for {distortion_type}")
            return self._fourier_analysis(x)

        # âœ… Histogram ë¶„ì„ (ë…¸ì¶œ ë° ë°ê¸° ë³€í™” ë¶„ì„)
        elif distortion_type in [
            20, 21, 51, 53, 56, 60
        ]:
            print(f"[Debug] Applying Histogram analysis for {distortion_type}")
            return self._histogram_analysis(x)

        else:
            print(f"[Warning] Unknown distortion type: {distortion_type}. Returning original input.")
            return x



    def _sobel_filter(self, x):
        """Sobel í•„í„° ì ìš© (Gradient Magnitude + Direction)"""
        c = x.size(1)
        sobel_x = torch.tensor([[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]], dtype=torch.float32).to(x.device).unsqueeze(0)
        sobel_y = torch.tensor([[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]], dtype=torch.float32).to(x.device).unsqueeze(0)

        if c > 1:
            sobel_x = sobel_x.repeat(c, 1, 1, 1)
            sobel_y = sobel_y.repeat(c, 1, 1, 1)

        grad_x = F.conv2d(x, sobel_x, padding=1, groups=c)
        grad_y = F.conv2d(x, sobel_y, padding=1, groups=c)

        # âœ… ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°(G) ê³„ì‚°
        gradient_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2)

        # âœ… ê·¸ë˜ë””ì–¸íŠ¸ ë°©í–¥(Î¸) ê³„ì‚° (ë¼ë””ì•ˆ ê°’, -Ï€ ~ Ï€ ë²”ìœ„)
        gradient_direction = torch.atan2(grad_y, grad_x)

        # âœ… ë°©í–¥ì„ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™” (ì›ë˜ -Ï€ ~ Ï€ ë²”ìœ„ì´ë¯€ë¡œ)
        gradient_direction_normalized = (gradient_direction + torch.pi) / (2 * torch.pi)

        return torch.sigmoid(gradient_magnitude.mean(dim=1, keepdim=True)), gradient_direction_normalized.mean(dim=1, keepdim=True)

    def _hsv_analysis(self, x):
        hsv = self._rgb_to_hsv(x)
        return hsv[:, 1:2, :, :]  # Saturation ì±„ë„ ë°˜í™˜
    
    def _histogram_analysis(self, x):
        hist_map = torch.mean(x, dim=1, keepdim=True)
        return F.normalize(hist_map, p=2, dim=[-2, -1])  # L2 ì •ê·œí™” ì¶”ê°€



    def _fourier_analysis(self, x):
        """
        í‘¸ë¦¬ì— ë³€í™˜(Fourier Transform) ì ìš©í•˜ì—¬ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ ì™œê³¡ ë¶„ì„
        ê³µê°„ ë„ë©”ì¸(í”½ì…€ë‹¨ìœ„ì •ë³´) -> ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜
        -> ì´ë¯¸ì§€ íŒ¨í„´ì´ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ

        (1) 2D í‘¸ë¦¬ì— ë³€í™˜ ì •ì˜:
            - ì´ì‚° í‘¸ë¦¬ì— ë³€í™˜(DFT)ì€ ì´ë¯¸ì§€ì˜ ê³µê°„ ë„ë©”ì¸(Spatial Domain)ì—ì„œ 
            ì£¼íŒŒìˆ˜ ë„ë©”ì¸(Frequency Domain)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹ í˜¸ ë¶„ì„ì„ ìˆ˜í–‰í•¨.

            - ì—¬ê¸°ì„œ:
                * f(x, y): ì›ë³¸ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’
                * F(u, v): ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ ë³€í™˜ëœ ê°’
                * (u, v): ì£¼íŒŒìˆ˜ ì¢Œí‘œ
                * (M, N): ì´ë¯¸ì§€ í¬ê¸°

        (2) í‘¸ë¦¬ì— ë³€í™˜ì„ í™œìš©í•œ ì™œê³¡ íƒì§€:
            - JPEG ì••ì¶• ì•„í‹°íŒ©íŠ¸(artifact), ë¸”ëŸ¬(blur) ë“±ì˜ ì™œê³¡ì„ ê°ì§€í•  ìˆ˜ ìˆìŒ.
            - ì €ì£¼íŒŒ ì„±ë¶„ (Low Frequency): ì´ë¯¸ì§€ì˜ ì „ì²´ì ì¸ êµ¬ì¡° ë° ë°ê¸° ì •ë³´.
            - ê³ ì£¼íŒŒ ì„±ë¶„ (High Frequency): ì„¸ë¶€ í…ìŠ¤ì²˜ ë° ê²½ê³„ ì •ë³´.
            - ë¸”ëŸ¬(Blur) ë° ì••ì¶• ë…¸ì´ì¦ˆ(Artifacts)ëŠ” ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ ê³ ì£¼íŒŒ ì„±ë¶„ì„ ê°ì†Œì‹œí‚¤ëŠ” íŠ¹ì§•ì´ ìˆìŒ.

        """

        # âœ… ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        h, w = x.shape[-2:]

        # âœ… 2ì˜ ê±°ë“­ì œê³± í¬ê¸°ë¡œ íŒ¨ë”©í•˜ì—¬ í‘¸ë¦¬ì— ë³€í™˜ ì„±ëŠ¥ ìµœì í™”
        new_h, new_w = 2 ** int(np.ceil(np.log2(h))), 2 ** int(np.ceil(np.log2(w)))
        padded_x = F.pad(x, (0, new_w - w, 0, new_h - h))

        # âœ… 2D í‘¸ë¦¬ì— ë³€í™˜ ìˆ˜í–‰ (ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜)
        fft = torch.fft.fft2(padded_x, dim=(-2, -1))

        # âœ… ì§„í­(Amplitude) ê³„ì‚° (ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì˜ íŠ¹ì • ì„±ë¶„ ê°•ì¡°)
        magnitude = torch.abs(fft)

        # âœ… ì§„í­ì„ 1ì±„ë„ë¡œ í‰ê·  ë‚´ì–´ ë°˜í™˜ í›„ sigmoid ì •ê·œí™” ì ìš©
        return torch.sigmoid(magnitude[:, :, :h, :w].mean(dim=1, keepdim=True))



    def _rgb_to_hsv(self, x):
        max_rgb, _ = x.max(dim=1, keepdim=True)
        min_rgb, _ = x.min(dim=1, keepdim=True)
        delta = max_rgb - min_rgb
        saturation = delta / torch.clamp(max_rgb, min=1e-6)
        value = max_rgb  # ì¶”ê°€ëœ value ì±„ë„
        return torch.cat((delta, saturation, value), dim=1)  # HSV 3ì±„ë„ ë°˜í™˜
    

""" class HardNegativeCrossAttention(nn.Module):
    def __init__(self, in_channels, num_heads=8):
        super(HardNegativeCrossAttention, self).__init__()

        if in_channels % num_heads != 0:
            num_heads = max(1, in_channels // 8)

        self.num_heads = num_heads
        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.output_proj = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)

        self.attribute_processor = AttributeFeatureProcessor(in_channels)
        self.texture_processor = TextureBlockProcessor(in_channels)

        # âœ… í•™ìŠµ ê°€ëŠ¥í•œ Î± ê°€ì¤‘ì¹˜ ì¶”ê°€
        self.alpha = nn.Parameter(torch.tensor(0.3))  # ì´ˆê¸°ê°’ 0.5 (0~1 ë²”ìœ„ í•™ìŠµ ê°€ëŠ¥)

        # âœ… Scale Factor ì ìš© (S)
        self.scale_factor = nn.Parameter(torch.tensor(1.0 / (in_channels ** 0.5)))

        self.layer_norm = nn.LayerNorm([in_channels, 1, 1])

    def forward(self, x_attr, x_texture):
        # âœ… Attribute Features ë³€í™˜
        x_attr = self.attribute_processor(x_attr)

        # âœ… Texture Features ë³€í™˜ í›„ ì¶”ê°€ ì—°ì‚° ì ìš©
        x_texture = self.texture_processor(x_texture)
        x_texture = x_texture * x_attr  # ğŸ”¥ **ê³±ì…ˆ (X ì—°ì‚°) ì¶”ê°€**

        if x_attr.size(2) != x_texture.size(2) or x_attr.size(3) != x_texture.size(3):
            min_h = min(x_attr.size(2), x_texture.size(2))
            min_w = min(x_attr.size(3), x_texture.size(3))
            x_attr = F.adaptive_avg_pool2d(x_attr, (min_h, min_w))
            x_texture = F.adaptive_avg_pool2d(x_texture, (min_h, min_w))

        b, c, h, w = x_attr.size()
        head_dim = c // self.num_heads

        query_attr = self.query_conv(x_attr).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)
        key_attr = self.key_conv(x_attr).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 2, 3)
        value_attr = self.value_conv(x_attr).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)

        query_tex = self.query_conv(x_texture).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)
        key_tex = self.key_conv(x_texture).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 2, 3)
        value_tex = self.value_conv(x_texture).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)

        scale = self.scale_factor.to(query_attr.device)

        # âœ… Attribute Feature Attention
        attention_attr = self.softmax(torch.matmul(query_attr, key_attr) * scale)
        A_attr = torch.matmul(attention_attr, value_attr).permute(0, 1, 3, 2).contiguous()

        # âœ… Texture Feature Attention
        attention_tex = self.softmax(torch.matmul(query_tex, key_tex) * scale)
        A_tex = torch.matmul(attention_tex, value_tex).permute(0, 1, 3, 2).contiguous()

        # âœ… **Î± ê°€ì¤‘ì¹˜ ì ìš©**
        A_final = self.alpha * A_attr + (1 - self.alpha) * A_tex  # ğŸ”¥ **Î± ì ìš©**

        A_final = A_final.view(b, c, h, w)
        A_final = self.output_proj(A_final)

        out = nn.Dropout(p=0.1)(A_final)

        if not hasattr(self, "layer_norm") or self.layer_norm.normalized_shape != (c, h, w):
            self.layer_norm = nn.LayerNorm([c, h, w]).to(out.device)

        out = self.layer_norm(out)
        return out """


class HardNegativeCrossAttention(nn.Module):
    def __init__(self, in_channels, num_heads=8):
        super(HardNegativeCrossAttention, self).__init__()

        self.num_heads = num_heads
        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.output_proj = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)

        self.attribute_processor = AttributeFeatureProcessor(in_channels)
        self.texture_processor = TextureBlockProcessor(in_channels)

        # âœ… í•™ìŠµ ê°€ëŠ¥í•œ Î± ê°€ì¤‘ì¹˜ ì¶”ê°€
        self.alpha = nn.Parameter(torch.tensor(0.5))  # ì´ˆê¸°ê°’ 0.3 (0~1 ë²”ìœ„ í•™ìŠµ ê°€ëŠ¥)

        self.scale_factor = nn.Parameter(torch.tensor(1.0 / (in_channels ** 0.5)))
        self.layer_norm = nn.LayerNorm([in_channels, 1, 1])

    def forward(self, x_attr, x_texture):
        # âœ… Attribute Features ë³€í™˜
        x_attr = self.attribute_processor(x_attr)

        # âœ… Texture Block Features ë³€í™˜ í›„ ì¶”ê°€ ì—°ì‚° ì ìš©
        x_texture = self.texture_processor(x_texture)

        # âœ… Transpose ì—°ì‚° ì¶”ê°€ (ë‹¤ì´ì–´ê·¸ë¨ê³¼ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •)
        x_texture = F.interpolate(x_texture, scale_factor=2, mode='bilinear', align_corners=False)

        # âœ… Î± ê°€ì¤‘ì¹˜ ì ìš©í•œ ì„ í˜• ì¡°í•©
        x_weighted = self.alpha * x_attr + (1 - self.alpha) * x_texture  # ğŸ”¥ Î± ì ìš©

        # âœ… Concatenation ì ìš© (C ì—°ì‚°)
        x_concat = torch.cat([x_weighted, x_texture], dim=1)  # ğŸ”¥ Î± ê°€ì¤‘ì¹˜ ì ìš© í›„ ê²°í•©

        # Attention ì ìš©
        b, c, h, w = x_concat.size()
        head_dim = c // self.num_heads

        query = self.query_conv(x_concat).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)
        key = self.key_conv(x_concat).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 2, 3)
        value = self.value_conv(x_concat).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)

        scale = self.scale_factor.to(query.device)
        attention = self.softmax(torch.matmul(query, key) * scale)
        A_final = torch.matmul(attention, value).permute(0, 1, 3, 2).contiguous()

        # âœ… Concatí•œ ë’¤ ìµœì¢… Projection ì ìš©
        A_final = A_final.view(b, c, h, w)
        A_final = self.output_proj(A_final)

        # âœ… Î± ê°€ì¤‘ì¹˜ ì ìš©í•œ Residual Connection
        out = self.layer_norm(A_final + x_weighted)  # ğŸ”¥ Î± ì ìš© í›„ Residual Connection

        return out

""" class HardNegativeCrossAttention(nn.Module):
    def __init__(self, in_channels, num_heads=8):
        super(HardNegativeCrossAttention, self).__init__()

        self.num_heads = num_heads
        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.output_proj = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)

        self.attribute_processor = AttributeFeatureProcessor(in_channels)
        self.texture_processor = TextureBlockProcessor(in_channels)

        # âœ… í•™ìŠµ ê°€ëŠ¥í•œ Î± ê°€ì¤‘ì¹˜ ì¶”ê°€ (ì´ˆê¸°ê°’ 0.7)
        self.alpha = nn.Parameter(torch.tensor(0.3))  

        self.scale_factor = nn.Parameter(torch.tensor(1.0 / (in_channels ** 0.5)))
        self.layer_norm = nn.LayerNorm([in_channels, 1, 1])

    def forward(self, x_attr, x_texture):
        # âœ… Attribute Features ë³€í™˜
        x_attr = self.attribute_processor(x_attr)

        # âœ… Texture Block Features ë³€í™˜ í›„ ì¶”ê°€ ì—°ì‚° ì ìš©
        x_texture = self.texture_processor(x_texture)

        # âœ… Transpose ì—°ì‚° ì¶”ê°€ (ë‹¤ì´ì–´ê·¸ë¨ê³¼ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •)
        x_texture = F.interpolate(x_texture, scale_factor=2, mode='bilinear', align_corners=False)

        # âœ… Î± ê°€ì¤‘ì¹˜ ì ìš©í•œ ì„ í˜• ì¡°í•© (í•™ìŠµ ê°€ëŠ¥)
        x_weighted = self.alpha * x_attr + (1 - self.alpha) * x_texture  # ğŸ”¥ Î± ì ìš©

        # âœ… Concatenation ì ìš© (C ì—°ì‚°)
        x_concat = torch.cat([x_weighted, x_texture], dim=1)  # ğŸ”¥ Î± ê°€ì¤‘ì¹˜ ì ìš© í›„ ê²°í•©

        # Attention ì ìš©
        b, c, h, w = x_concat.size()
        head_dim = c // self.num_heads

        query = self.query_conv(x_concat).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)
        key = self.key_conv(x_concat).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 2, 3)
        value = self.value_conv(x_concat).view(b, self.num_heads, head_dim, h * w).permute(0, 1, 3, 2)

        scale = self.scale_factor.to(query.device)
        attention = self.softmax(torch.matmul(query, key) * scale)
        A_final = torch.matmul(attention, value).permute(0, 1, 3, 2).contiguous()

        # âœ… Concatí•œ ë’¤ ìµœì¢… Projection ì ìš©
        A_final = A_final.view(b, c, h, w)
        A_final = self.output_proj(A_final)

        # âœ… Î± ê°€ì¤‘ì¹˜ ì ìš©í•œ Residual Connection
        out = self.layer_norm(A_final + x_weighted)  # ğŸ”¥ Î± ì ìš© í›„ Residual Connection

        return out """




# ì‹œê°í™” í•¨ìˆ˜1
def visualize_distortion_classification(input_image, distortion_logits):
    distortion_probs = torch.softmax(distortion_logits, dim=1).detach().cpu().numpy()[0]

    # distortion_labels í¬ê¸°ë¥¼ distortion_probs ê°œìˆ˜ì™€ ë§ì¶¤
    #distortion_labels = list(distortion_map.keys())[:len(distortion_probs)]  # ê°œìˆ˜ ì¼ì¹˜

    distortion_labels = list(distortion_map.keys())
    if len(distortion_labels) != len(distortion_probs):
        distortion_labels = distortion_labels[:len(distortion_probs)]

    plt.figure(figsize=(12, 6))
    
    # ì…ë ¥ ì´ë¯¸ì§€ ì‹œê°í™”
    plt.subplot(1, 2, 1)
    plt.imshow(input_image)
    plt.title("Input Image")
    plt.axis("off")

    # ì™œê³¡ ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”
    plt.subplot(1, 2, 2)
    plt.bar(distortion_labels, distortion_probs)
    plt.title("Distortion Classification Probabilities")
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()


# í‘¸ë¦¬ì— ì‹œê°í™”
""" def apply_fft(image_tensor):
    fft = torch.fft.fft2(image_tensor, dim=(-2, -1))
    magnitude = torch.abs(fft)  # ì§„í­(Amplitude) ê³„ì‚°
    log_magnitude = torch.log1p(magnitude)  # ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³€í™˜ (ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° í¸í•˜ë„ë¡)
    return log_magnitude

def high_pass_filter(fft, cutoff=10):
    h, w = fft.shape[-2:]
    center_h, center_w = h // 2, w // 2

    mask = torch.ones_like(fft)  # ì „ì²´ë¥¼ 1ë¡œ ì´ˆê¸°í™”
    mask[:, :, center_h - cutoff:center_h + cutoff, center_w - cutoff:center_w + cutoff] = 0  # ì €ì£¼íŒŒ ì œê±°

    return fft * mask  # ê³ ì£¼íŒŒ ë¶€ë¶„ë§Œ ë‚¨ê¹€

def low_pass_filter(fft, cutoff=10):
    h, w = fft.shape[-2:]
    center_h, center_w = h // 2, w // 2

    mask = torch.zeros_like(fft)  # ì „ì²´ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    mask[:, :, center_h - cutoff:center_h + cutoff, center_w - cutoff:center_w + cutoff] = 1  # ì €ì£¼íŒŒ ìœ ì§€

    return fft * mask  # ì €ì£¼íŒŒ ë¶€ë¶„ë§Œ ë‚¨ê¹€

def apply_ifft(fft):
    return torch.fft.ifft2(fft, dim=(-2, -1)).real

def visualize_fourier(image_path, cutoff=10):
    
    # âœ… ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
    image = Image.open(image_path).convert("L")  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    image_tensor = transform(image).unsqueeze(0)  # (1, 1, 224, 224)

    # âœ… í‘¸ë¦¬ì— ë³€í™˜ ìˆ˜í–‰
    fft = torch.fft.fft2(image_tensor, dim=(-2, -1))

    # âœ… ê³ ì£¼íŒŒ ë° ì €ì£¼íŒŒ í•„í„° ì ìš©
    high_pass_fft = high_pass_filter(fft, cutoff)
    low_pass_fft = low_pass_filter(fft, cutoff)

    # âœ… ì—­ í‘¸ë¦¬ì— ë³€í™˜ ìˆ˜í–‰
    high_pass_reconstructed = apply_ifft(high_pass_fft)
    low_pass_reconstructed = apply_ifft(low_pass_fft)

    # âœ… ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë³€í™˜
    log_fft = apply_fft(image_tensor)
    log_high_pass = apply_fft(high_pass_fft)
    log_low_pass = apply_fft(low_pass_fft)

    # âœ… ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    fig, ax = plt.subplots(2, 3, figsize=(12, 8))

    ax[0, 0].imshow(image, cmap="gray")
    ax[0, 0].set_title("ì›ë³¸ ì´ë¯¸ì§€ (Spatial Domain)")

    ax[0, 1].imshow(log_fft.squeeze().numpy(), cmap="gray")
    ax[0, 1].set_title("í‘¸ë¦¬ì— ë³€í™˜ ê²°ê³¼ (Frequency Domain)")

    ax[0, 2].imshow(log_high_pass.squeeze().numpy(), cmap="gray")
    ax[0, 2].set_title("ê³ ì£¼íŒŒ ê°•ì¡° (High-Pass Filtering)")

    ax[1, 0].imshow(log_low_pass.squeeze().numpy(), cmap="gray")
    ax[1, 0].set_title("ì €ì£¼íŒŒ ê°•ì¡° (Low-Pass Filtering)")

    ax[1, 1].imshow(high_pass_reconstructed.squeeze().numpy(), cmap="gray")
    ax[1, 1].set_title("ì—­ í‘¸ë¦¬ì— ë³€í™˜ (High-Pass)")

    ax[1, 2].imshow(low_pass_reconstructed.squeeze().numpy(), cmap="gray")
    ax[1, 2].set_title("ì—­ í‘¸ë¦¬ì— ë³€í™˜ (Low-Pass)")

    plt.tight_layout()
    plt.show()

# âœ… ì‹¤í–‰: ë…¼ë¬¸ì— ë„£ì„ ì‹œê°í™” ìƒì„±
image_path = "E:/ARNIQA - SE - mix/ARNIQA/dataset/CSIQ/dst_imgs/BLUR/1600.BLUR.5.png"  # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
visualize_fourier(image_path, cutoff=20)

 """

# Main Execution for Debugging
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„ì˜ ì…ë ¥ ë°ì´í„°
    batch_size = 32
    in_channels = 2048
    height, width = 7, 7  # ì…ë ¥ í¬ê¸°

        # ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
    input_image_path = r"E:\ARNIQA - SE - mix\ARNIQA\dataset\KONIQ10K\1024x768\11706252.jpg"
    input_image = Image.open(input_image_path).convert("RGB")
    input_tensor = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor()
    ])(input_image).unsqueeze(1)  # 1ì±„ë„ -> 3ì±„ë„ í™•ì¥ í•„ìš”

    # DistortionAttention ëª¨ë¸ ì´ˆê¸°í™”
    in_channels = 3  # RGB 3ì±„ë„ë¡œ ì„¤ì •
    distortion_attention = DistortionAttention(in_channels=in_channels)

    # DistortionAttention ì ìš©
    with torch.no_grad():
        output, distortion_logits = distortion_attention(input_tensor.repeat(1, 3, 1, 1))  # 1ì±„ë„ â†’ 3ì±„ë„ í™•ì¥

    # ê²°ê³¼ ì‹œê°í™”
    visualize_distortion_classification(np.array(input_image), distortion_logits)




    x_attr = torch.randn(batch_size, in_channels, height, width)  # Attribute Features
    x_texture = torch.randn(batch_size, in_channels, height, width)  # Texture Block Features

    print(f"Input x_attr shape: {x_attr.shape}")
    print(f"Input x_texture shape: {x_texture.shape}")

    # HardNegativeCrossAttention ì´ˆê¸°í™”
    hnca = HardNegativeCrossAttention(in_channels=in_channels, num_heads=8)

    # Forward Pass
    output = hnca(x_attr, x_texture)
    print("HardNegativeCrossAttention Output shape:", output.shape)

    # Global Average Pooling ì ìš©
    global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
    pooled_output = global_avg_pool(output)
    print("Global Avg Pool Output shape:", pooled_output.shape)

    # Projection Head
    proj_head = nn.Linear(in_channels, 128)
    final_output = proj_head(pooled_output.view(batch_size, -1))
    print("Projection Head Output shape:", final_output.shape)