import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from torchvision import transforms
import cv2
import seaborn as sns
import scipy.stats


class FeatureLevelAttention(nn.Module):
    def __init__(self, in_channels=3, out_channels=32, num_heads=8, dropout=0.3):
        super(FeatureLevelAttention, self).__init__()

        self.num_heads = num_heads
        self.out_channels = out_channels

        # âœ… ì…ë ¥ ì±„ë„ ë³€í™˜ (3 â†’ 32)
        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

        assert out_channels % num_heads == 0, "out_channels must be divisible by num_heads"
        self.head_dim = max(out_channels // num_heads, 1)
        self.scale = self.head_dim ** -0.5  # Scale Dot-Product Attention

        # Query, Key, Value ìƒì„±
        self.qkv_conv = nn.Conv2d(out_channels, out_channels * 3, kernel_size=1, bias=False)
        self.output_proj = nn.Conv2d(out_channels, out_channels, kernel_size=1)

        # Feature Update (Residual Learning + Normalization)
        self.feature_update_conv = nn.Conv2d(out_channels, out_channels, kernel_size=1)
        self.batch_norm = nn.BatchNorm2d(out_channels)

        self.activation = nn.SiLU()
        self.dropout = nn.Dropout(dropout)

        # âœ… ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì ìš©
        self._initialize_weights()

    def _initialize_weights(self):
        """ Xavier Uniform ì´ˆê¸°í™” ì ìš© """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        b, c, h, w = x.size()

        # âœ… ì…ë ¥ ì±„ë„ ë³€í™˜ (3ì±„ë„ â†’ 32ì±„ë„)
        x = self.input_conv(x)

        # Query, Key, Value ìƒì„±
        qkv = self.qkv_conv(x).reshape(b, self.num_heads, 3, self.head_dim, h * w)
        q, k, v = torch.chunk(qkv, 3, dim=2)
        q, k, v = q.squeeze(2), k.squeeze(2), v.squeeze(2)

        # âœ… Scaled Dot-Product Attention ëŒ€ì‹  Sigmoid ì ìš© (ë¶„í¬ ê°œì„ )
        attention = torch.sigmoid(torch.matmul(q, k.transpose(-2, -1)) * self.scale)
        A_final = torch.matmul(attention, v)
        A_final = A_final.reshape(b, self.out_channels, h, w)

        # ìµœì¢… Projection ì ìš©
        A_final = self.output_proj(A_final)
        A_final = self.feature_update_conv(A_final)
        A_final = self.batch_norm(A_final)
        A_final = self.activation(A_final)
        A_final = self.dropout(A_final)

        # Residual Connection ì ìš©
        out = A_final + x

        return out, attention


def visualize_attention(img_path, model):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])

    image = Image.open(img_path).convert("RGB")
    img_tensor = transform(image).unsqueeze(0)  # (1, 3, 224, 224)

    with torch.no_grad():
        _, attention_map = model(img_tensor)

    # âœ… Attention Map í¬ê¸° í™•ì¸ ë° ì¡°ì •
    print(f"Original Attention Map Shape: {attention_map.shape}")  # (1, 8, 4, 4)

    # ğŸ”¥ (1, 8, 4, 4) â†’ (1, 4, 4)ë¡œ í‰ê· í™” í›„ Interpolation ì ìš©
    attention_map = attention_map.mean(dim=1, keepdim=True)  # (1, 1, 4, 4)
    attention_map = torch.nn.functional.interpolate(
        attention_map, size=(224, 224), mode="bilinear", align_corners=False
    ).squeeze(0)  # (1, 224, 224)

    # Attention Map í›„ì²˜ë¦¬
    attention_map = attention_map.squeeze().cpu().numpy()
    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())

    # âœ… Attention Mapì˜ ê¸°ë³¸ í†µê³„ ì¶œë ¥
    print(f"Attention Map Mean: {attention_map.mean():.4f}")
    print(f"Attention Map Std Dev: {attention_map.std():.4f}")
    print(f"Attention Map Min: {attention_map.min():.4f}")
    print(f"Attention Map Max: {attention_map.max():.4f}")

    # âœ… Attention Map íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”
    plt.figure(figsize=(6, 4))
    sns.histplot(attention_map.flatten(), bins=30, kde=True)
    plt.title("Distribution of Attention Weights")
    plt.xlabel("Attention Score")
    plt.ylabel("Frequency")
    plt.show()

    # âœ… ì›ë³¸ ì´ë¯¸ì§€ ë³€í™˜ (Grayscaleì¸ì§€ í™•ì¸ í›„ ë³€í™˜)
    img_cv = cv2.imread(img_path)

    # ğŸ”¥ img_cvê°€ grayscale(1ì±„ë„)ì¸ ê²½ìš° 3ì±„ë„ë¡œ ë³€í™˜
    if len(img_cv.shape) == 2 or img_cv.shape[-1] == 1:
        img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)

    img_cv = cv2.resize(img_cv, (224, 224))

    # âœ… Attention Mapì„ 3ì±„ë„ë¡œ ë³€í™˜ í›„ ColorMap ì ìš©
    heatmap = cv2.applyColorMap(np.uint8(255 * attention_map), cv2.COLORMAP_JET)

    # ğŸ”¥ í¬ê¸°ì™€ ì±„ë„ ìˆ˜ê°€ ê°™ì•„ì•¼ OpenCV addWeighted ì‚¬ìš© ê°€ëŠ¥
    if img_cv.shape != heatmap.shape:
        heatmap = cv2.resize(heatmap, (img_cv.shape[1], img_cv.shape[0]))

    # âœ… ê°€ì¤‘ì¹˜ í•©ì„± (ë‘ ì´ë¯¸ì§€ í¬ê¸°/ì±„ë„ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸ í›„ ì ìš©)
    overlay = cv2.addWeighted(img_cv, 0.6, heatmap, 0.4, 0)

    # âœ… Attention Mapê³¼ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ ê°’ì˜ ìƒê´€ ê´€ê³„ ê³„ì‚°
    image_gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY) / 255.0  # Normalize to 0-1
    image_gray_resized = cv2.resize(image_gray, (attention_map.shape[1], attention_map.shape[0]))

    corr, p_value = scipy.stats.pearsonr(image_gray_resized.flatten(), attention_map.flatten())
    print(f"Correlation between Image Intensity and Attention: {corr:.4f}")

    # ì‹œê°í™”
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    plt.title("Original Image")

    plt.subplot(1, 3, 2)
    plt.imshow(attention_map, cmap="jet")
    plt.title("Attention Map")

    plt.subplot(1, 3, 3)
    plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))
    plt.title("Overlayed Image")

    plt.show()


# ì‹¤í–‰
if __name__ == "__main__":
    num_heads = 8
    feature_attention = FeatureLevelAttention(in_channels=3, out_channels=32, num_heads=num_heads)

    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
    test_img_path = "E:/ARNIQA - SE - mix/ARNIQA/dataset/CSIQ/dst_imgs/AWGN/family.AWGN.5.png"

    # Attention Map ì‹œê°í™” ì‹¤í–‰
    visualize_attention(test_img_path, feature_attention)
