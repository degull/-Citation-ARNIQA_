""" import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from tqdm import tqdm
from pathlib import Path
from scipy import stats
from data.dataset_koniq10k import KONIQ10KDataset
from models.attention_se import EnhancedDistortionDetectionModel
from utils.utils import load_config

# âœ… ì†ì‹¤ í•¨ìˆ˜ (MSE + Perceptual Loss)
def distortion_loss(pred, gt):
    mse_loss = nn.MSELoss()(pred, gt)
    perceptual_loss = torch.mean(torch.abs(pred - gt))
    return mse_loss + 0.1 * perceptual_loss

# âœ… SROCC ë° PLCC ê³„ì‚°
def calculate_srcc_plcc(preds, targets):
    preds, targets = preds.cpu().numpy(), targets.cpu().numpy()
    srocc, _ = stats.spearmanr(preds.flatten(), targets.flatten())
    plcc, _ = stats.pearsonr(preds.flatten(), targets.flatten())
    return srocc, plcc

# âœ… í•™ìŠµ ë£¨í”„
def train(args, model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device):
    best_srocc = -1
    model.train()

    train_losses = []
    val_srocc_values, val_plcc_values = [], []

    for epoch in range(args.training.epochs):
        running_loss = 0.0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch + 1}/{args.training.epochs}]")

        for batch in progress_bar:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            optimizer.zero_grad()

            # âœ… ëª¨ë¸ ì˜ˆì¸¡
            preds = model(img_A)

            # âœ… ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°
            loss = distortion_loss(preds, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            progress_bar.set_postfix(loss=running_loss / (len(progress_bar) + 1))

        avg_loss = running_loss / len(train_dataloader)
        train_losses.append(avg_loss)

        # âœ… ê²€ì¦
        val_srocc, val_plcc = validate(model, val_dataloader, device)
        val_srocc_values.append(val_srocc)
        val_plcc_values.append(val_plcc)

        # âœ… ëª¨ë¸ ì €ì¥
        if val_srocc > best_srocc:
            best_srocc = val_srocc
            save_checkpoint(model, args.checkpoint_base_path, epoch, val_srocc)

        print(f"\nğŸ”¹ Epoch {epoch+1}: Loss: {avg_loss:.6f}, Val SROCC: {val_srocc:.6f}, Val PLCC: {val_plcc:.6f}")

        lr_scheduler.step()

    print("\nâœ… **Training Completed** âœ…")

    return {
        "loss": train_losses,
        "srocc": val_srocc_values,
        "plcc": val_plcc_values
    }

# âœ… ê²€ì¦ ë£¨í”„
def validate(model, dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            preds = model(img_A)
            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return np.mean(srocc_values), np.mean(plcc_values)

# âœ… í…ŒìŠ¤íŠ¸ ë£¨í”„ (ì¶”ê°€)
def test(model, test_dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in test_dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            preds = model(img_A)
            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return {
        "srocc": srocc_values,
        "plcc": plcc_values
    }

# âœ… ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_checkpoint(model, checkpoint_path, epoch, srocc):
    filename = f"epoch_{epoch}_srocc_{srocc:.3f}.pth"
    torch.save(model.state_dict(), Path(checkpoint_path) / filename)



# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    # âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ
    config_path = "E:/ARNIQA - SE - mix/ARNIQA/config.yaml"
    args = load_config(config_path)

    # âœ… GPU ì„¤ì •
    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")

    # âœ… ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_path = Path(args.data_base_path)
    dataset = KONIQ10KDataset(str(dataset_path), crop_size=224)


    train_size = int(0.7 * len(dataset))
    val_size = int(0.1 * len(dataset))
    test_size = len(dataset) - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

    train_dataloader = DataLoader(train_dataset, batch_size=args.training.batch_size, shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)
    test_dataloader = DataLoader(test_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)

    # âœ… ëª¨ë¸ ìƒì„±
    model = EnhancedDistortionDetectionModel().to(device)

    # âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = optim.SGD(model.parameters(), lr=args.training.learning_rate, momentum=0.9, weight_decay=1e-4)
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)

    # âœ… í•™ìŠµ ì‹œì‘
    train_metrics = train(args, model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device)

    # âœ… í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    test_metrics = test(model, test_dataloader, device)

    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\nâœ… **Training Completed** âœ…\n")

    print("ğŸ”¹ **Final Training Metrics:** ğŸ”¹")
    for epoch, (loss, srocc, plcc) in enumerate(zip(train_metrics["loss"], train_metrics["srocc"], train_metrics["plcc"])):
        print(f"ğŸ“Œ **Epoch {epoch+1}:** Loss: {loss:.6f}, SROCC: {srocc:.6f}, PLCC: {plcc:.6f}")

    print("\nğŸ”¹ **Final Validation Metrics:** ğŸ”¹", {
        "srocc": train_metrics["srocc"],
        "plcc": train_metrics["plcc"]
    })

    print("ğŸ”¹ **Final Test Metrics:** ğŸ”¹", test_metrics)

 """

""" import torch.optim as optim
from utils.utils import load_config
import io
import torch
from torch.utils.data import DataLoader, random_split
import numpy as np
from tqdm import tqdm
from pathlib import Path
from scipy import stats
import torch.nn as nn
import numpy as np
from dotmap import DotMap
from pathlib import Path
from scipy import stats
from tqdm import tqdm
from data import KADID10KDataset, KONIQ10KDataset
import matplotlib.pyplot as plt
import random
from typing import Tuple
import argparse
import yaml
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from tqdm import tqdm
from pathlib import Path
import random
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from models.attention_se import EnhancedDistortionDetectionModel


# Config loader
def load_config(config_path: str) -> DotMap:
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
    return DotMap(config)

def save_checkpoint(model: nn.Module, checkpoint_path: Path, epoch: int, srocc: float) -> None:
    filename = f"epoch_{epoch}_srocc_{srocc:.3f}.pth"
    torch.save(model.state_dict(), checkpoint_path / filename)

def calculate_srcc_plcc(proj_A, proj_B):
    proj_A = proj_A.detach().cpu().numpy()
    proj_B = proj_B.detach().cpu().numpy()
    assert proj_A.shape == proj_B.shape, "Shape mismatch between proj_A and proj_B"
    srocc, _ = stats.spearmanr(proj_A.flatten(), proj_B.flatten())
    plcc, _ = stats.pearsonr(proj_A.flatten(), proj_B.flatten())
    return srocc, plcc


# âœ… ê²€ì¦ í•¨ìˆ˜ (img_B ì œê±°)
def validate(model, dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in dataloader:
            img_A = batch["img_A"].to(device)  # img_B ì œê±°
            mos = batch["mos"].to(device)  

            preds = model(img_A)

            srocc, plcc = calculate_srcc_plcc(preds, mos)
            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return np.mean(srocc_values), np.mean(plcc_values)

# âœ… í•™ìŠµ í•¨ìˆ˜ (img_B ì œê±°)
def train(args, model, train_dataloader, val_dataloader, test_dataloader, optimizer, lr_scheduler, device):
    checkpoint_path = Path(str(args.checkpoint_base_path))
    checkpoint_path.mkdir(parents=True, exist_ok=True)
    best_srocc = -1

    train_metrics = {'loss': []}
    val_metrics = {'srcc': [], 'plcc': []}
    test_metrics_per_epoch = []

    for epoch in range(args.training.epochs):
        model.train()
        running_loss = 0.0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch + 1}/{args.training.epochs}]")

        for batch in progress_bar:
            img_A = batch["img_A"].to(device)  # img_B ì œê±°
            mos = batch["mos"].to(device)

            optimizer.zero_grad()

            # âœ… ëª¨ë¸ ì˜ˆì¸¡
            preds = model(img_A)

            # âœ… MSE Loss ê³„ì‚°
            loss = F.mse_loss(preds, mos)

            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            progress_bar.set_postfix(loss=running_loss / (len(progress_bar) + 1))

        avg_loss = running_loss / len(train_dataloader)
        train_metrics['loss'].append(avg_loss)

        # âœ… ê²€ì¦
        val_srocc, val_plcc = validate(model, val_dataloader, device)
        val_metrics['srcc'].append(val_srocc)
        val_metrics['plcc'].append(val_plcc)

        # âœ… í…ŒìŠ¤íŠ¸
        test_srocc, test_plcc = validate(model, test_dataloader, device)
        test_metrics_per_epoch.append({'srcc': test_srocc, 'plcc': test_plcc})

        print(f"Epoch {epoch + 1}: Test SRCC = {test_srocc:.4f}, PLCC = {test_plcc:.4f}")
        lr_scheduler.step()

        if val_srocc > best_srocc:
            best_srocc = val_srocc
            save_checkpoint(model, checkpoint_path, epoch, val_srocc)

    print("Training completed.")

    print("\nFinal Test Metrics Per Epoch:")
    for i, metrics in enumerate(test_metrics_per_epoch, 1):
        print(f"Epoch {i}: SRCC = {metrics['srcc']:.4f}, PLCC = {metrics['plcc']:.4f}")

    return train_metrics, val_metrics, test_metrics_per_epoch


def test(args, model, test_dataloader, device):
    if test_dataloader is None:
        raise ValueError("Test DataLoaderê°€ Noneì…ë‹ˆë‹¤. ì´ˆê¸°í™” ë¬¸ì œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in test_dataloader:
            inputs_A = batch["img_A"].to(device)
            inputs_B = batch["img_B"].to(device)

            # Flatten crops if needed (5D -> 4D)
            if inputs_A.dim() == 5:
                inputs_A = inputs_A.view(-1, *inputs_A.shape[2:])
            if inputs_B.dim() == 5:
                inputs_B = inputs_B.view(-1, *inputs_B.shape[2:])

            proj_A, proj_B = model(inputs_A, inputs_B)

            # Normalize projections
            proj_A = F.normalize(proj_A, dim=1).cpu().numpy()
            proj_B = F.normalize(proj_B, dim=1).cpu().numpy()

            # Calculate SRCC and PLCC
            srocc, _ = stats.spearmanr(proj_A.flatten(), proj_B.flatten())
            plcc, _ = stats.pearsonr(proj_A.flatten(), proj_B.flatten())

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    avg_srocc_test = np.mean(srocc_values)
    avg_plcc_test = np.mean(plcc_values)
    return {'srcc': avg_srocc_test, 'plcc': avg_plcc_test}



# âœ… ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    config_path = "E:/ARNIQA - SE - mix/ARNIQA/config.yaml"
    args = load_config(config_path)

    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")

    # âœ… ë°ì´í„°ì…‹ ë¡œë“œ (KADID10K: Train, KONIQ10K: Test)
    kadid_dataset = KADID10KDataset(str(args.data_base_path_kadid))
    koniq_dataset = KONIQ10KDataset(str(args.data_base_path_koniq))

    # âœ… ë°ì´í„°ì…‹ ë¶„í• 
    train_size = int(0.8 * len(kadid_dataset))
    val_size = len(kadid_dataset) - train_size
    train_dataset, val_dataset = random_split(kadid_dataset, [train_size, val_size])

    train_dataloader = DataLoader(train_dataset, batch_size=args.training.batch_size, shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)
    test_dataloader = DataLoader(koniq_dataset, batch_size=args.test.batch_size, shuffle=False, num_workers=4)

    # âœ… ëª¨ë¸ ì •ì˜
    model = EnhancedDistortionDetectionModel().to(device)

    # âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = optim.Adam(model.parameters(), lr=args.training.learning_rate, weight_decay=args.training.optimizer.weight_decay)
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)

    # âœ… ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    train_metrics, val_metrics, test_metrics = train(
        args, model, train_dataloader, val_dataloader, test_dataloader, optimizer, lr_scheduler, device
    )

    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\nğŸ”¹ **Final Test Metrics Per Epoch:** ğŸ”¹")
    for i, metrics in enumerate(test_metrics, 1):
        print(f"ğŸ“Œ Epoch {i}: SRCC = {metrics['srcc']:.4f}, PLCC = {metrics['plcc']:.4f}") """


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from tqdm import tqdm
from pathlib import Path
from scipy import stats
from data.dataset_kadid10k import KADID10KDataset
from data.dataset_csiq import CSIQDataset
from models.attention_se import EnhancedDistortionDetectionModel
from utils.utils import load_config

# âœ… ì†ì‹¤ í•¨ìˆ˜ (MSE + Perceptual Loss)
def distortion_loss(pred, gt):
    mse_loss = nn.MSELoss()(pred, gt)
    perceptual_loss = torch.mean(torch.abs(pred - gt))
    return mse_loss + 0.1 * perceptual_loss

# âœ… SROCC ë° PLCC ê³„ì‚°
def calculate_srcc_plcc(preds, targets):
    preds, targets = preds.cpu().numpy(), targets.cpu().numpy()
    srocc, _ = stats.spearmanr(preds.flatten(), targets.flatten())
    plcc, _ = stats.pearsonr(preds.flatten(), targets.flatten())
    return srocc, plcc

# âœ… í•™ìŠµ ë£¨í”„ (KADID10Kë¡œ í›ˆë ¨)
def train(args, model, train_dataloader, val_dataloader, test_dataloader, optimizer, lr_scheduler, device):
    best_srocc = -1
    model.train()

    train_losses = []
    val_srocc_values, val_plcc_values = [], []
    test_srocc_values, test_plcc_values = [], []

    for epoch in range(args.training.epochs):
        running_loss = 0.0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch + 1}/{args.training.epochs}]")

        for batch in progress_bar:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            optimizer.zero_grad()

            # âœ… ëª¨ë¸ ì˜ˆì¸¡
            preds = model(img_A)

            # âœ… ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°
            loss = distortion_loss(preds, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            progress_bar.set_postfix(loss=running_loss / (len(progress_bar) + 1))

        avg_loss = running_loss / len(train_dataloader)
        train_losses.append(avg_loss)

        # âœ… ê²€ì¦ (KADID10K Validation)
        val_srocc, val_plcc = validate(model, val_dataloader, device)
        val_srocc_values.append(val_srocc)
        val_plcc_values.append(val_plcc)

        # âœ… í…ŒìŠ¤íŠ¸ (CSIQ Dataset)
        test_srocc, test_plcc = test(model, test_dataloader, device)
        test_srocc_values.append(test_srocc)
        test_plcc_values.append(test_plcc)

        # âœ… ëª¨ë¸ ì €ì¥
        if val_srocc > best_srocc:
            best_srocc = val_srocc
            save_checkpoint(model, args.checkpoint_base_path, epoch, val_srocc)

        print(f"\nğŸ”¹ Epoch {epoch+1}: Loss: {avg_loss:.6f}, Val SROCC: {val_srocc:.6f}, Val PLCC: {val_plcc:.6f}")
        print(f"ğŸ“Œ Test CSIQ: SROCC: {test_srocc:.6f}, PLCC: {test_plcc:.6f}")

        lr_scheduler.step()

    print("\nâœ… **Training Completed** âœ…")

    return {
        "loss": train_losses,
        "val_srocc": val_srocc_values,
        "val_plcc": val_plcc_values,
        "test_srocc": test_srocc_values,
        "test_plcc": test_plcc_values
    }

# âœ… ê²€ì¦ ë£¨í”„ (KADID10K Validation Set)
def validate(model, dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            preds = model(img_A)
            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return np.mean(srocc_values), np.mean(plcc_values)

# âœ… í…ŒìŠ¤íŠ¸ ë£¨í”„ (CSIQ ë°ì´í„°ì…‹)
def test(model, test_dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in test_dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            preds = model(img_A)
            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return np.mean(srocc_values), np.mean(plcc_values)

# âœ… ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_checkpoint(model, checkpoint_path, epoch, srocc):
    filename = f"epoch_{epoch}_srocc_{srocc:.3f}.pth"
    torch.save(model.state_dict(), Path(checkpoint_path) / filename)

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    # âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ
    config_path = "E:/ARNIQA - SE - mix/ARNIQA/config.yaml"
    args = load_config(config_path)

    # âœ… GPU ì„¤ì •
    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")

    # âœ… ë°ì´í„°ì…‹ ë¡œë“œ (KADID10K Train, CSIQ Test)
    kadid_dataset = KADID10KDataset(str(args.data_base_path_kadid))
    csiq_dataset = CSIQDataset(str(args.data_base_path_csiq))

    # âœ… ë°ì´í„°ì…‹ ë¶„í• 
    train_size = int(0.7 * len(kadid_dataset))
    val_size = len(kadid_dataset) - train_size

    train_dataset, val_dataset = random_split(kadid_dataset, [train_size, val_size])

    train_dataloader = DataLoader(train_dataset, batch_size=args.training.batch_size, shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)
    test_dataloader = DataLoader(csiq_dataset, batch_size=args.test.batch_size, shuffle=False, num_workers=4)

    # âœ… ëª¨ë¸ ìƒì„±
    model = EnhancedDistortionDetectionModel().to(device)

    # âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = optim.Adam(model.parameters(), lr=args.training.learning_rate, weight_decay=1e-4)
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)

    # âœ… í•™ìŠµ ì‹œì‘
    train_metrics = train(args, model, train_dataloader, val_dataloader, test_dataloader, optimizer, lr_scheduler, device)

    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\nâœ… **Training Completed** âœ…\n")

    print("ğŸ”¹ **Final Training Metrics:** ğŸ”¹")
    for epoch, (loss, srocc, plcc) in enumerate(zip(train_metrics["loss"], train_metrics["test_srocc"], train_metrics["test_plcc"])):
        print(f"ğŸ“Œ **Epoch {epoch+1}:** Loss: {loss:.6f}, Test CSIQ SROCC: {srocc:.6f}, Test CSIQ PLCC: {plcc:.6f}")
