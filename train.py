import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from tqdm import tqdm
from pathlib import Path
from scipy import stats
from data.dataset_kadid10k import KADID10KDataset
from models.attention_se import EnhancedDistortionDetectionModel
from utils.utils import load_config

# âœ… ì†ì‹¤ í•¨ìˆ˜ (MSE + Perceptual Loss)
def distortion_loss(pred, gt):
    mse_loss = nn.MSELoss()(pred, gt)
    perceptual_loss = torch.mean(torch.abs(pred - gt))
    return mse_loss + 0.1 * perceptual_loss

# âœ… SROCC ë° PLCC ê³„ì‚°
def calculate_srcc_plcc(preds, targets):
    preds, targets = preds.cpu().numpy(), targets.cpu().numpy()
    srocc, _ = stats.spearmanr(preds.flatten(), targets.flatten())
    plcc, _ = stats.pearsonr(preds.flatten(), targets.flatten())
    return srocc, plcc

# âœ… í•™ìŠµ ë£¨í”„
def train(args, model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device):
    best_srocc = -1
    model.train()

    train_losses = []
    val_srocc_values, val_plcc_values = [], []

    for epoch in range(args.training.epochs):
        running_loss = 0.0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch + 1}/{args.training.epochs}]")

        for batch in progress_bar:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            optimizer.zero_grad()

            # âœ… ëª¨ë¸ ì˜ˆì¸¡
            preds = model(img_A)

            # âœ… ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°
            loss = distortion_loss(preds, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            progress_bar.set_postfix(loss=running_loss / (len(progress_bar) + 1))

        avg_loss = running_loss / len(train_dataloader)
        train_losses.append(avg_loss)

        # âœ… ê²€ì¦
        val_srocc, val_plcc = validate(model, val_dataloader, device)
        val_srocc_values.append(val_srocc)
        val_plcc_values.append(val_plcc)

        # âœ… ëª¨ë¸ ì €ì¥
        if val_srocc > best_srocc:
            best_srocc = val_srocc
            save_checkpoint(model, args.checkpoint_base_path, epoch, val_srocc)

        print(f"\nğŸ”¹ Epoch {epoch+1}: Loss: {avg_loss:.6f}, Val SROCC: {val_srocc:.6f}, Val PLCC: {val_plcc:.6f}")

        lr_scheduler.step()

    print("\nâœ… **Training Completed** âœ…")

    return {
        "loss": train_losses,
        "srocc": val_srocc_values,
        "plcc": val_plcc_values
    }

# âœ… ê²€ì¦ ë£¨í”„
def validate(model, dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            preds = model(img_A)
            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return np.mean(srocc_values), np.mean(plcc_values)

# âœ… í…ŒìŠ¤íŠ¸ ë£¨í”„ (ì¶”ê°€)
def test(model, test_dataloader, device):
    model.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in test_dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            preds = model(img_A)
            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return {
        "srocc": srocc_values,
        "plcc": plcc_values
    }

# âœ… ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_checkpoint(model, checkpoint_path, epoch, srocc):
    filename = f"epoch_{epoch}_srocc_{srocc:.3f}.pth"
    torch.save(model.state_dict(), Path(checkpoint_path) / filename)



# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    # âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ
    config_path = "E:/ARNIQA - SE - mix/ARNIQA/config.yaml"
    args = load_config(config_path)

    # âœ… GPU ì„¤ì •
    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")

    # âœ… ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_path = Path(args.data_base_path)
    dataset = KADID10KDataset(str(dataset_path), crop_size=224)


    train_size = int(0.7 * len(dataset))
    val_size = int(0.1 * len(dataset))
    test_size = len(dataset) - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

    train_dataloader = DataLoader(train_dataset, batch_size=args.training.batch_size, shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)
    test_dataloader = DataLoader(test_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)

    # âœ… ëª¨ë¸ ìƒì„±
    model = EnhancedDistortionDetectionModel().to(device)

    # âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = optim.SGD(model.parameters(), lr=args.training.learning_rate, momentum=0.9, weight_decay=1e-4)
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)

    # âœ… í•™ìŠµ ì‹œì‘
    train_metrics = train(args, model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device)

    # âœ… í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    test_metrics = test(model, test_dataloader, device)

    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\nâœ… **Training Completed** âœ…\n")

    print("ğŸ”¹ **Final Training Metrics:** ğŸ”¹")
    for epoch, (loss, srocc, plcc) in enumerate(zip(train_metrics["loss"], train_metrics["srocc"], train_metrics["plcc"])):
        print(f"ğŸ“Œ **Epoch {epoch+1}:** Loss: {loss:.6f}, SROCC: {srocc:.6f}, PLCC: {plcc:.6f}")

    print("\nğŸ”¹ **Final Validation Metrics:** ğŸ”¹", {
        "srocc": train_metrics["srocc"],
        "plcc": train_metrics["plcc"]
    })

    print("ğŸ”¹ **Final Test Metrics:** ğŸ”¹", test_metrics)

    
""" 
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
from pathlib import Path
from scipy import stats
from tqdm import tqdm
from utils.utils import load_config
from data.dataset_kadid10k import KADID10KDataset
from data.dataset_csiq import CSIQDataset
from models.attention_se import EnhancedDistortionDetectionModel  # âœ… Feature Extractor

# âœ… Regressor ì¶”ê°€
class IQARegressor(nn.Module):
    def __init__(self, feature_dim=512):
        super(IQARegressor, self).__init__()
        self.regressor = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # ìµœì¢… MOS ì˜ˆì¸¡ê°’ ì¶œë ¥
        )

    def forward(self, x):
        return self.regressor(x)

# âœ… ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_checkpoint(model, checkpoint_path, epoch, srocc):
    filename = f"epoch_{epoch}_srocc_{srocc:.3f}.pth"
    torch.save(model.state_dict(), checkpoint_path / filename)

# âœ… SROCC ë° PLCC ê³„ì‚° í•¨ìˆ˜
def calculate_srcc_plcc(preds, targets):
    if torch.isnan(preds).any() or torch.isnan(targets).any():
        return np.nan, np.nan  # NaNì´ í¬í•¨ëœ ê²½ìš° NaN ë°˜í™˜
    
    preds, targets = preds.cpu().numpy(), targets.cpu().numpy()
    srocc, _ = stats.spearmanr(preds.flatten(), targets.flatten())
    plcc, _ = stats.pearsonr(preds.flatten(), targets.flatten())
    return srocc, plcc

# âœ… ê²€ì¦ ë£¨í”„
def validate(feature_extractor, regressor, dataloader, device):
    feature_extractor.eval()
    regressor.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            features = feature_extractor(img_A)
            preds = regressor(features).squeeze()

            # âœ… NaN ì²´í¬ ë° í•„í„°ë§
            if torch.isnan(preds).any() or torch.isnan(targets).any():
                print("[Warning] NaN detected in validation batch. Skipping...")
                continue

            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return np.nanmean(srocc_values), np.nanmean(plcc_values)

# âœ… í•™ìŠµ ë£¨í”„ (lr_schedulerì´ Noneì´ë©´ step() í˜¸ì¶œ ì•ˆí•¨)
def train(args, feature_extractor, regressor, train_dataloader, val_dataloader, test_dataloader, optimizer, lr_scheduler, device):
    checkpoint_path = Path(str(args.checkpoint_base_path))
    checkpoint_path.mkdir(parents=True, exist_ok=True)
    best_srocc = -1

    train_metrics = {'loss': []}
    val_metrics = {'srcc': [], 'plcc': []}
    test_metrics_per_epoch = []  

    for epoch in range(args.training.epochs):
        feature_extractor.train()
        regressor.train()
        running_loss = 0.0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch + 1}/{args.training.epochs}]")

        for batch in progress_bar:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            optimizer.zero_grad()

            features = feature_extractor(img_A)
            preds = regressor(features).squeeze()

            if torch.isnan(preds).any() or torch.isnan(targets).any():
                print("[Warning] NaN detected in batch. Skipping...")
                continue

            preds = torch.clamp(preds, min=0.0, max=1.0)
            targets = torch.clamp(targets, min=0.0, max=1.0)

            loss = nn.MSELoss()(preds, targets)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            progress_bar.set_postfix(loss=running_loss / (len(progress_bar) + 1))

        avg_loss = running_loss / len(train_dataloader)
        train_metrics['loss'].append(avg_loss)

        val_srocc, val_plcc = validate(feature_extractor, regressor, val_dataloader, device)
        val_metrics['srcc'].append(val_srocc)
        val_metrics['plcc'].append(val_plcc)

        test_metrics = test(feature_extractor, regressor, test_dataloader, device)
        test_metrics_per_epoch.append(test_metrics)

        avg_srcc = np.nanmean(test_metrics['srcc'])
        avg_plcc = np.nanmean(test_metrics['plcc'])
        print(f"Epoch {epoch + 1}: Test SRCC = {avg_srcc:.4f}, PLCC = {avg_plcc:.4f}")

        # âœ… lr_schedulerì´ Noneì´ ì•„ë‹ˆë©´ step() ì‹¤í–‰
        if lr_scheduler is not None:
            lr_scheduler.step()

        if val_srocc > best_srocc:
            best_srocc = val_srocc
            save_checkpoint(regressor, checkpoint_path, epoch, val_srocc)

    print("Training completed.")

    print("\nFinal Test Metrics Per Epoch:")
    for i, metrics in enumerate(test_metrics_per_epoch, 1):
        avg_srcc = np.nanmean(metrics['srcc'])
        avg_plcc = np.nanmean(metrics['plcc'])
        print(f"Epoch {i}: SRCC = {avg_srcc:.4f}, PLCC = {avg_plcc:.4f}")

    return train_metrics, val_metrics, test_metrics_per_epoch



# âœ… í…ŒìŠ¤íŠ¸ ë£¨í”„
def test(feature_extractor, regressor, test_dataloader, device):
    feature_extractor.eval()
    regressor.eval()
    srocc_values, plcc_values = [], []

    with torch.no_grad():
        for batch in test_dataloader:
            img_A = batch["img_A"].to(device)
            targets = batch["mos"].to(device)

            features = feature_extractor(img_A)
            preds = regressor(features).squeeze()

            # âœ… NaN ì²´í¬
            if torch.isnan(preds).any() or torch.isnan(targets).any():
                print("[Warning] NaN detected in test batch. Skipping...")
                continue

            srocc, plcc = calculate_srcc_plcc(preds, targets)

            srocc_values.append(srocc)
            plcc_values.append(plcc)

    return {"srcc": np.nanmean(srocc_values), "plcc": np.nanmean(plcc_values)}


# âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ
def create_dataloaders(args):
    kadid_dataset_path = Path(str(args.data_base_path_kadid))
    csiq_dataset_path = Path(str(args.data_base_path_csiq))

    kadid_dataset = KADID10KDataset(str(kadid_dataset_path))
    csiq_dataset = CSIQDataset(str(csiq_dataset_path))

    train_size = int(0.8 * len(kadid_dataset))
    val_size = len(kadid_dataset) - train_size
    train_dataset, val_dataset = random_split(kadid_dataset, [train_size, val_size])

    train_dataloader = DataLoader(train_dataset, batch_size=args.training.batch_size, shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=args.training.batch_size, shuffle=False, num_workers=4)
    test_dataloader = DataLoader(csiq_dataset, batch_size=args.test.batch_size, shuffle=False, num_workers=4)

    return train_dataloader, val_dataloader, test_dataloader

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    config_path = "E:/ARNIQA - SE - mix/ARNIQA/config.yaml"
    args = load_config(config_path)

    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")

    train_dataloader, val_dataloader, test_dataloader = create_dataloaders(args)

    feature_extractor = EnhancedDistortionDetectionModel().to(device)
    regressor = IQARegressor(feature_dim=feature_extractor.feature_dim).to(device)

    optimizer = optim.Adam(
        list(feature_extractor.parameters()) + list(regressor.parameters()), 
        lr=args.training.learning_rate
    )

    # âœ… lr_scheduler ì¶”ê°€
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=args.training.lr_scheduler.T_0,
        T_mult=args.training.lr_scheduler.T_mult,
        eta_min=args.training.lr_scheduler.eta_min
    ) if hasattr(args.training, "lr_scheduler") else None  # â— `args.training.lr_scheduler`ê°€ ì—†ìœ¼ë©´ None í• ë‹¹

    train_metrics, val_metrics, test_metrics = train(
        args, feature_extractor, regressor, train_dataloader, val_dataloader, test_dataloader, optimizer, lr_scheduler, device
    ) """